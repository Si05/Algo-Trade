{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'not', 'a', 'sentimental', 'person', 'but', 'I', 'believe', 'in', 'the', 'utility', 'of', 'sentiment', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "text = \"I am not a sentimental person but I believe in the utility of sentiment analysis\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'not',\n",
       " 'a',\n",
       " 'sentimental',\n",
       " 'person',\n",
       " 'but',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'in',\n",
       " 'the',\n",
       " 'utility',\n",
       " 'of',\n",
       " 'sentiment',\n",
       " 'analysis']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'not', 'a', 'sentiment', 'person', 'but', 'I', 'believ', 'in', 'the', 'util', 'of', 'sentiment', 'analysi']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "okens=word_tokenize(text.lower())\n",
    "ps = PorterStemmer()\n",
    "tokens=[ps.stem(word) for word in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'sentiment', 'person', 'I', 'believ', 'util', 'sentiment', 'analysi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_new = [j for j in tokens if j not in stopwords]\n",
    "tokens_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Based Approach\n",
    "* Relies on an underlying sentiment (or opinion) lexion\n",
    "* A sentiment lexicon is a list of lexical features (e.g. words) which are generally labeled according to their semantic orientation as either positive or negative\n",
    "* Manually creating and validating such lists of opinion-bearing features, while being among the most robust methods, is also one of the most time-consuming\n",
    "* Much of the applied research leveraging sentiment analysis relies heavily on preexisting manually constructed lexicons\n",
    "    * LICW\n",
    "    * ANEW\n",
    "    * SentiWordNet\n",
    "    * SenticNet\n",
    "    * VADER\n",
    "* `Drawback` of using Lexicons to perform sentiment analysis includes:\n",
    "    * Tend to suffer from inability to process acronyms, initialism, emotioncons, slangs etc. and therefore perform poorly on social media text data\n",
    "    * They are unable to account for sentiment intensity\n",
    "    * Unable to process sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "`VADER is a lexison developed by Georgia Tech CS Department, which addresses some of the above shortcomings`\n",
    "* Has incorporated popular slangs (LOL, OMG, ROFL, Nah, Meh, etc.) and Emoticons (🙂😐🙁, etc.)\n",
    "* Features are rates on a scale of -4 (extremely negative) to +4 (extremely positive) thereby factoring in sentiment intensity\n",
    "* Has had very successful tests on social media data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Criteria`\n",
    "1. positive sentiment: compound score >= 0.5\n",
    "2. neutral sentiment: -0.5 < compound score < 0.5\n",
    "3. negative sentiment: compound score <= -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "analyser.polarity_scores(\"This is a good course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.6249}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"This is an awesome course\") # degree modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.572, 'pos': 0.428, 'compound': 0.4572}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"The instructor is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.6696}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"The instructor is so cool!!\") # exclaimataion changes score\n",
    "analyser.polarity_scores(\"The instructor is so COOL!!\") # Capitalization changes score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.4588}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"Machine learning makes me :)\") #emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.618, 'neu': 0.382, 'pos': 0.0, 'compound': -0.4995}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser.polarity_scores(\"His antics had me ROFL\")\n",
    "analyser.polarity_scores(\"The movie SUX\") #Slangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "`Using pre-built lexicon WordNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"His\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.75, subjectivity=0.75)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"remarkable\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.875, subjectivity=0.875)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"His remarkable work ethic impressed me\").sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Sentiment Analyzer using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinlinsi/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "date_time = []\n",
    "news_text = []\n",
    "headlines = [] \n",
    "\n",
    "for i in range(1,3): #parameters of range function correspond to page numbers in the website with news listings\n",
    "    #get the list of unique urls in the page\n",
    "    url = 'https://oilprice.com/Energy/Crude-Oil/Page-{}.html'.format(i)\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "    for links in soup.find_all('div', {'class': 'categoryArticle'}):\n",
    "        for info in links.find_all('a'):\n",
    "            if info.get('href') not in url_list:\n",
    "                url_list.append(info.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oilprice.com/Energy/Crude-Oil/US-Oil-Defies-Odds-Races-Towards-Annual-Production-Record.html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for www in url_list:\n",
    "    #access each url\n",
    "    headlines.append(www.split(\"/\")[-1].replace('-',' ').replace('.html', ''))\n",
    "    request = requests.get(www)\n",
    "    soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "    \n",
    "    #store date and time of publication of the article\n",
    "    for dates in soup.find_all('span', {'class': 'article_byline'}):\n",
    "        date_time.append(dates.text.split('-')[-1])\n",
    "    \n",
    "    #store the text of the news\n",
    "    temp = []\n",
    "    for news in soup.find_all('p'):\n",
    "            temp.append(news.text)\n",
    "    \n",
    "    #identify the last line of the news article\n",
    "    for last_sentence in reversed(temp):\n",
    "        if last_sentence.split(\" \")[0]==\"By\" and last_sentence.split(\" \")[-1]==\"Oilprice.com\":\n",
    "            break\n",
    "        elif last_sentence.split(\" \")[0]==\"By\":\n",
    "            break\n",
    "    \n",
    "    #prune non news related text from the scraped data to create the news text\n",
    "    joined_text = ' '.join(temp[temp.index(\"More Info\")+1:temp.index(last_sentence)])\n",
    "    news_text.append(joined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save news text along with the news headline in a dataframe      \n",
    "news_df = pd.DataFrame({ 'Date' : date_time,\n",
    "                         'Headline': headlines,\n",
    "                         'News': news_text,\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use VADER to perform sentiment analysis on stored news articles\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def comp_score(text):\n",
    "    return analyser.polarity_scores(text)[\"compound\"]   \n",
    "  \n",
    "news_df[\"sentiment\"] = news_df[\"News\"].apply(comp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>News</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oct 13, 2023, 6:00 PM CDT</td>\n",
       "      <td>Kuwait And Saudi Arabia Team Up For Massive Ga...</td>\n",
       "      <td>As other Middle Eastern states look to diversi...</td>\n",
       "      <td>0.9971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oct 12, 2023, 3:00 PM CDT</td>\n",
       "      <td>US Oil Defies Odds Races Towards Annual Produc...</td>\n",
       "      <td>One of my 2023 energy sector predictions was t...</td>\n",
       "      <td>0.3877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oct 12, 2023, 10:08 AM CDT</td>\n",
       "      <td>Oil Moves Down On Massive Inventory Build</td>\n",
       "      <td>Crude oil prices saw a small dip on Thursday m...</td>\n",
       "      <td>-0.8868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oct 11, 2023, 7:00 PM CDT</td>\n",
       "      <td>Shift In US Policy On Iran Oil Could Swing Glo...</td>\n",
       "      <td>Back in August, we reported that Iran oil expo...</td>\n",
       "      <td>-0.8818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oct 11, 2023, 6:00 PM CDT</td>\n",
       "      <td>Shale Consolidation Could Put A Permanent Lid ...</td>\n",
       "      <td>Back in April, the Wall Street Journal reporte...</td>\n",
       "      <td>0.9938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date  \\\n",
       "0    Oct 13, 2023, 6:00 PM CDT   \n",
       "1    Oct 12, 2023, 3:00 PM CDT   \n",
       "2   Oct 12, 2023, 10:08 AM CDT   \n",
       "3    Oct 11, 2023, 7:00 PM CDT   \n",
       "4    Oct 11, 2023, 6:00 PM CDT   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Kuwait And Saudi Arabia Team Up For Massive Ga...   \n",
       "1  US Oil Defies Odds Races Towards Annual Produc...   \n",
       "2          Oil Moves Down On Massive Inventory Build   \n",
       "3  Shift In US Policy On Iran Oil Could Swing Glo...   \n",
       "4  Shale Consolidation Could Put A Permanent Lid ...   \n",
       "\n",
       "                                                News  sentiment  \n",
       "0  As other Middle Eastern states look to diversi...     0.9971  \n",
       "1  One of my 2023 energy sector predictions was t...     0.3877  \n",
       "2  Crude oil prices saw a small dip on Thursday m...    -0.8868  \n",
       "3  Back in August, we reported that Iran oil expo...    -0.8818  \n",
       "4  Back in April, the Wall Street Journal reporte...     0.9938  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[['Headline', 'News', 'sentiment']].copy().to_csv('news_df.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "Assign weighting factor used to get the important features from the documents\n",
    "\n",
    "**TF** = $\\frac{number\\ of\\ times\\ the\\ term\\ t\\ appear\\ in\\ the\\ doc}{total\\ number\\ of\\ words\\ in\\ the\\ doc}$\n",
    "\n",
    "**IDF** = $log_{e}\\big(\\frac{total\\ number\\ of\\ documents}{number\\ of\\ documents\\ with\\ the\\ term\\ t\\ in\\ it}\\big)$\n",
    "\n",
    "**TF-IDF** = $TF\\ \\times\\ IDF$\n",
    "\n",
    "* TF-IDF is used to improve the feature set before feeding into ML model\n",
    "* It penalized common words and reduces their weights in the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinlinsi/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>News</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kuwait And Saudi Arabia Team Up For Massive Ga...</td>\n",
       "      <td>As other Middle Eastern states look to diversi...</td>\n",
       "      <td>0.9971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US Oil Defies Odds Races Towards Annual Produc...</td>\n",
       "      <td>One of my 2023 energy sector predictions was t...</td>\n",
       "      <td>0.3877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil Moves Down On Massive Inventory Build</td>\n",
       "      <td>Crude oil prices saw a small dip on Thursday m...</td>\n",
       "      <td>-0.8868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shift In US Policy On Iran Oil Could Swing Glo...</td>\n",
       "      <td>Back in August, we reported that Iran oil expo...</td>\n",
       "      <td>-0.8818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shale Consolidation Could Put A Permanent Lid ...</td>\n",
       "      <td>Back in April, the Wall Street Journal reporte...</td>\n",
       "      <td>0.9938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Kuwait And Saudi Arabia Team Up For Massive Ga...   \n",
       "1  US Oil Defies Odds Races Towards Annual Produc...   \n",
       "2          Oil Moves Down On Massive Inventory Build   \n",
       "3  Shift In US Policy On Iran Oil Could Swing Glo...   \n",
       "4  Shale Consolidation Could Put A Permanent Lid ...   \n",
       "\n",
       "                                                News  sentiment  \n",
       "0  As other Middle Eastern states look to diversi...     0.9971  \n",
       "1  One of my 2023 energy sector predictions was t...     0.3877  \n",
       "2  Crude oil prices saw a small dip on Thursday m...    -0.8868  \n",
       "3  Back in August, we reported that Iran oil expo...    -0.8818  \n",
       "4  Back in April, the Wall Street Journal reporte...     0.9938  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(x):\n",
    "    if x > 0.5:\n",
    "        label = 'Positive'\n",
    "    elif x < -0.5:\n",
    "        label = 'Negative'\n",
    "    else:\n",
    "        label = 'Neutral'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    As other Middle Eastern states look to diversi...\n",
       "1    One of my 2023 energy sector predictions was t...\n",
       "2    Crude oil prices saw a small dip on Thursday m...\n",
       "3    Back in August, we reported that Iran oil expo...\n",
       "4    Back in April, the Wall Street Journal reporte...\n",
       "Name: News, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"news_df.csv\")\n",
    "data = data[~data['News'].isna()].copy()\n",
    "data['label'] = data.apply(lambda x: label_sentiment(x['sentiment']), axis = 1)\n",
    "X = data.iloc[:,1] \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "X_vec = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2290)\t2\n",
      "  (0, 1268)\t1\n",
      "  (0, 3439)\t1\n",
      "  (0, 2170)\t1\n",
      "  (0, 1195)\t1\n",
      "  (0, 1327)\t2\n",
      "  (0, 2325)\t1\n",
      "  (0, 2059)\t20\n",
      "  (0, 408)\t1\n",
      "  (0, 1668)\t1\n",
      "  (0, 3646)\t1\n",
      "  (0, 2470)\t16\n",
      "  (0, 1631)\t13\n",
      "  (0, 362)\t1\n",
      "  (0, 2060)\t1\n",
      "  (0, 1674)\t4\n",
      "  (0, 387)\t1\n",
      "  (0, 326)\t1\n",
      "  (0, 2204)\t2\n",
      "  (0, 2486)\t6\n",
      "  (0, 2772)\t3\n",
      "  (0, 2242)\t1\n",
      "  (0, 609)\t2\n",
      "  (0, 2777)\t11\n",
      "  (0, 816)\t2\n",
      "  :\t:\n",
      "  (0, 570)\t1\n",
      "  (0, 440)\t1\n",
      "  (0, 2832)\t1\n",
      "  (0, 1117)\t1\n",
      "  (0, 3914)\t1\n",
      "  (0, 3821)\t1\n",
      "  (0, 1758)\t1\n",
      "  (0, 569)\t1\n",
      "  (0, 1960)\t1\n",
      "  (0, 1750)\t1\n",
      "  (0, 1622)\t1\n",
      "  (0, 1899)\t2\n",
      "  (0, 2396)\t1\n",
      "  (0, 1957)\t1\n",
      "  (0, 1193)\t1\n",
      "  (0, 3944)\t1\n",
      "  (0, 2707)\t1\n",
      "  (0, 1419)\t1\n",
      "  (0, 1390)\t1\n",
      "  (0, 564)\t1\n",
      "  (0, 2169)\t1\n",
      "  (0, 2970)\t1\n",
      "  (0, 3591)\t1\n",
      "  (0, 1034)\t1\n",
      "  (0, 294)\t2\n"
     ]
    }
   ],
   "source": [
    "print(X_vec[0])\n",
    "## (0, 2290)  2. <-- indicates the word \"middle\" (2290 is the token for \"middle\") appearing in article 1 twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 3, 0, ..., 0, 1, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 5, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(vectorizer, open(\"vectorizer_crude_oil\", 'wb'))  # Save vectorizor for reuse\n",
    "X_vec = X_vec.todense() # Convert sparse matrix into dense matrix\n",
    "X_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 3978)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data by applying term frequency inverse document frequency (TF-IDF) \n",
    "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
    "X_tfidf = tfidf.fit_transform(X_vec)\n",
    "X_tfidf = X_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the news body and labels for training the classifier\n",
    "X_train = X_tfidf[:,:]\n",
    "Y_train = data.iloc[:, 3]\n",
    "\n",
    "# Train the NB classifier\n",
    "clf = GaussianNB().fit(X_train, Y_train) \n",
    "pickle.dump(clf, open(\"nb_clf_crude_oil\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############Importing trained classifier and fitted vectorizer################\n",
    "nb_clf = pickle.load(open(\"nb_clf_crude_oil\", 'rb'))\n",
    "vectorizer = pickle.load(open(\"vectorizer_crude_oil\", 'rb'))\n",
    "\n",
    "nb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############Predict sentiment using the trained classifier###################\n",
    "# Import test data set\n",
    "data_pred = pd.read_csv(\"news_df.csv\")  ## Need a new test set\n",
    "X_test = data_pred[~data_pred['News'].isna()].iloc[:,1] # extract column with news articl\n",
    "X_vec_test = vectorizer.transform(X_test) #don't use fit_transform here because the model is already fitted\n",
    "X_vec_test = X_vec_test.todense() #convert sparse matrix to dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Neutral', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative'], dtype='<U8')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform data by applying term frequency inverse document frequency (TF-IDF) \n",
    "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
    "X_tfidf_test = tfidf.fit_transform(X_vec_test)\n",
    "X_tfidf_test = X_tfidf_test.todense()\n",
    "\n",
    "\n",
    "# Predict the sentiment values\n",
    "y_pred = nb_clf.predict(X_tfidf_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
